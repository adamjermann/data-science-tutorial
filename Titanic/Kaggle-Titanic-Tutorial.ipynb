{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing libs for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 12 columns):\n",
      "PassengerId    891 non-null int64\n",
      "Survived       891 non-null int64\n",
      "Pclass         891 non-null int64\n",
      "Name           891 non-null object\n",
      "Sex            891 non-null object\n",
      "Age            714 non-null float64\n",
      "SibSp          891 non-null int64\n",
      "Parch          891 non-null int64\n",
      "Ticket         891 non-null object\n",
      "Fare           891 non-null float64\n",
      "Cabin          204 non-null object\n",
      "Embarked       889 non-null object\n",
      "dtypes: float64(2), int64(5), object(5)\n",
      "memory usage: 83.6+ KB\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"titanic_train.csv\")\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    data.drop(['Cabin','Ticket'], axis=1, inplace=True)\n",
    "    def name_extract(word):\n",
    "        return word.split(',')[1].split('.')[0].strip()\n",
    "    salutation = pd.DataFrame({'Salutation': data['Name'].apply(name_extract)})\n",
    "    def group_salutation(old_salutation):\n",
    "        if old_salutation == 'Mr':\n",
    "            return('Mr')\n",
    "        else:\n",
    "            if old_salutation == 'Mrs':\n",
    "                return('Mrs')\n",
    "            else:\n",
    "                if old_salutation == 'Master':\n",
    "                    return('Master')\n",
    "                else: \n",
    "                    if old_salutation == 'Miss':\n",
    "                        return('Miss')\n",
    "                    else:\n",
    "                        if old_salutation == 'Master':\n",
    "                            return('Master')\n",
    "                        else:\n",
    "                            return('Others')\n",
    "    salutation = pd.DataFrame({'Salutation': salutation['Salutation'].apply(group_salutation)})\n",
    "    data = pd.merge(data, salutation, left_index = True, right_index = True)\n",
    "    table = data.pivot_table(values='Age', index=['Salutation'], columns=['Pclass', 'Sex'], aggfunc=np.mean)\n",
    "    def fage(x):\n",
    "        return table[x['Pclass']][x['Sex']][x['Salutation']]\n",
    "    data['Age'].fillna(data[data['Age'].isnull()].apply(fage, axis=1), inplace=True)\n",
    "    data.dropna(inplace=True)\n",
    "    data.ix[data.Fare > 200, 'Fare'] = 200\n",
    "    data['Family_size'] = data['SibSp'] + data['Parch'] + 1\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    le = LabelEncoder()\n",
    "    data.Embarked = le.fit_transform(data.Embarked)\n",
    "    data.Salutation = le.fit_transform(data.Salutation)\n",
    "    data.Sex = le.fit_transform(data.Sex)\n",
    "    from sklearn.preprocessing import OneHotEncoder\n",
    "    enc = OneHotEncoder(categorical_features='all')\n",
    "    data_encoded = enc.fit_transform(data[['Embarked','Salutation','Pclass']]).toarray()\n",
    "    data_encoded = pd.DataFrame(data_encoded)\n",
    "    data = pd.merge(data, data_encoded, left_index = True, right_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = data.drop(['PassengerId','Survived', 'Name', 'Pclass', 'Embarked', 'Salutation', 'Parch', 'SibSp', 7, 5, 10, 0, 4, 9], axis=1)\n",
    "y = data.ix[:, 'Survived']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "clf = DecisionTreeClassifier()\n",
    "cross_val_score(clf, X, y, cv=10).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "criterion = ['gini', 'entropy']\n",
    "splitter = ['best', 'random']\n",
    "max_depth = np.arange(1,10)\n",
    "min_samples_leaf = np.arange(1,20,2)\n",
    "min_samples_split = np.arange(1,20,2)\n",
    "\n",
    "parameters = {'criterion': criterion, 'splitter': splitter, 'max_depth': max_depth, \n",
    "              'min_samples_leaf': min_samples_leaf, 'min_samples_split': min_samples_split}\n",
    "clf = DecisionTreeClassifier(random_state=0)\n",
    "#clf = GridSearchCV(clf, parameters, cv=10)\n",
    "#clf.fit(X, y)\n",
    "\n",
    "#print clf.best_score_\n",
    "#print clf.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 418 entries, 0 to 417\n",
      "Data columns (total 11 columns):\n",
      "PassengerId    418 non-null int64\n",
      "Pclass         418 non-null int64\n",
      "Name           418 non-null object\n",
      "Sex            418 non-null object\n",
      "Age            332 non-null float64\n",
      "SibSp          418 non-null int64\n",
      "Parch          418 non-null int64\n",
      "Ticket         418 non-null object\n",
      "Fare           417 non-null float64\n",
      "Cabin          91 non-null object\n",
      "Embarked       418 non-null object\n",
      "dtypes: float64(2), int64(4), object(5)\n",
      "memory usage: 36.0+ KB\n"
     ]
    }
   ],
   "source": [
    "data_test = pd.read_csv(\"titanic_test.csv\")\n",
    "data_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "    data_test.drop(['Cabin','Ticket'], axis=1, inplace=True)\n",
    "    def name_extract(word):\n",
    "        return word.split(',')[1].split('.')[0].strip()\n",
    "    salutation = pd.DataFrame({'Salutation': data_test['Name'].apply(name_extract)})\n",
    "    def group_salutation(old_salutation):\n",
    "        if old_salutation == 'Mr':\n",
    "            return('Mr')\n",
    "        else:\n",
    "            if old_salutation == 'Mrs':\n",
    "                return('Mrs')\n",
    "            else:\n",
    "                if old_salutation == 'Master':\n",
    "                    return('Master')\n",
    "                else: \n",
    "                    if old_salutation == 'Miss':\n",
    "                        return('Miss')\n",
    "                    else:\n",
    "                        if old_salutation == 'Master':\n",
    "                            return('Master')\n",
    "                        else:\n",
    "                            return('Others')\n",
    "    salutation = pd.DataFrame({'Salutation': salutation['Salutation'].apply(group_salutation)})\n",
    "    data_test = pd.merge(data_test, salutation, left_index = True, right_index = True)\n",
    "    #print data.shape\n",
    "    #print data.isnull().sum()\n",
    "    #print data.Salutation.value_counts()\n",
    "    data_test['Age'].fillna(data_test[data_test['Age'].isnull()].apply(fage, axis=1), inplace=True)\n",
    "    #print data[data.isnull().any(axis=1)]\n",
    "    data_test['Fare'].fillna(data_test['Fare'].mean(), inplace=True)\n",
    "    data_test['Age'].fillna(data_test['Age'].mean(), inplace=True)\n",
    "    #print data.shape\n",
    "    #print data.isnull().sum()\n",
    "    data_test.dropna(inplace=True)\n",
    "    #print data.shape\n",
    "    data_test.ix[data_test.Fare > 200, 'Fare'] = 200\n",
    "    data_test['Family_size'] = data_test['SibSp'] + data_test['Parch'] + 1\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    le = LabelEncoder()\n",
    "    data_test.Embarked = le.fit_transform(data_test.Embarked)\n",
    "    data_test.Salutation = le.fit_transform(data_test.Salutation)\n",
    "    data_test.Sex = le.fit_transform(data_test.Sex)\n",
    "    from sklearn.preprocessing import OneHotEncoder\n",
    "    enc = OneHotEncoder(categorical_features='all')\n",
    "    data_encoded = enc.fit_transform(data_test[['Embarked','Salutation','Pclass']]).toarray()\n",
    "    data_encoded = pd.DataFrame(data_encoded)\n",
    "    data_test = pd.merge(data_test, data_encoded, left_index = True, right_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_test = data_test.drop(['PassengerId', 'Name', 'Pclass', 'Embarked', 'Salutation',  'Parch', 'SibSp', 7, 5, 10, 0, 4, 9], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DecisionTreeClassifier' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-d2249304262f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDecisionTreeClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin_samples_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplitter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'best'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'entropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_samples_leaf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'DecisionTreeClassifier' is not defined"
     ]
    }
   ],
   "source": [
    "clf = DecisionTreeClassifier(min_samples_split=5, splitter='best', criterion='entropy', max_depth=6, min_samples_leaf=1)\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predicted = pd.Series(clf.predict(X_test))\n",
    "solution = pd.concat([data_test['PassengerId'], predicted], axis=1)\n",
    "solution.columns = ['PassengerId','Survived']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "cv = 10\n",
    "\n",
    "clf = DecisionTreeClassifier(random_state=0)\n",
    "scores = cross_val_score(clf, X, y, cv=cv)\n",
    "print \"Decision Tree Score: %.3f\" % scores.mean()\n",
    "print \"Decision Tree Variance: %.4f \\n\" % scores.std()\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=10, max_depth=None, min_samples_split=1, random_state=0)\n",
    "scores = cross_val_score(clf, X, y, cv=cv)\n",
    "print \"Random Forest Score: %.3f\" % scores.mean()\n",
    "print \"Random Forest Variance: %.4f \\n\" % scores.std()\n",
    "\n",
    "clf = ExtraTreesClassifier(n_estimators=10, max_depth=None, min_samples_split=1, random_state=0)\n",
    "scores = cross_val_score(clf, X, y, cv=cv)\n",
    "print \"Extremely Randomized Trees Score: %.3f\" % scores.mean()\n",
    "print \"Extremely Randomized Trees Variance: %.4f \\n\" % scores.std()\n",
    "\n",
    "clf = AdaBoostClassifier(n_estimators=100, random_state=0)\n",
    "scores = cross_val_score(clf, X, y, cv=cv)\n",
    "print \"AdaBoost Score: %.3f\" % scores.mean()\n",
    "print \"AdaBoost Variance: %.4f \\n\" % scores.std()\n",
    "\n",
    "clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=0)\n",
    "scores = cross_val_score(clf, X, y, cv=cv)\n",
    "print \"Gradient Tree Boosting Score: %.3f\" % scores.mean()\n",
    "print \"Gradient Tree Boosting Variance: %.4f \\n\" % scores.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#criterion = ['friedman_mse', 'mse']\n",
    "learning_rate = np.arange(0.1,1,.1)\n",
    "loss = ['deviance', 'exponential']\n",
    "max_depth = np.arange(1,10)\n",
    "min_samples_split = np.arange(1,10,2)\n",
    "\n",
    "parameters = {'learning_rate': learning_rate, 'loss': loss, \"max_depth\": max_depth, \"min_samples_split\": min_samples_split}\n",
    "clf = RandomForestClassifier(random_state=0)\n",
    "clf = GridSearchCV(clf, parameters, cv=10)\n",
    "clf.fit(X, y)\n",
    "\n",
    "print clf.best_score_\n",
    "print clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "criterion = ['friedman_mse', 'mse']\n",
    "learning_rate = np.arange(0.1,1,.1)\n",
    "loss = ['deviance', 'exponential']\n",
    "max_depth = np.arange(1,10)\n",
    "min_samples_split = np.arange(1,10,2)\n",
    "\n",
    "parameters = {'learning_rate': learning_rate, 'loss': loss, \"max_depth\": max_depth, \"min_samples_split\": min_samples_split}\n",
    "clf = RandomForestClassifier(random_state=0)\n",
    "clf = GridSearchCV(clf, parameters, cv=10)\n",
    "clf.fit(X, y)\n",
    "\n",
    "print clf.best_score_\n",
    "print clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(min_samples_split=5, criterion='entropy', max_depth=6, min_samples_leaf=1)\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = GradientBoostingClassifier()\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clf.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "importances = clf.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "feature_order = X.columns[indices]\n",
    "\n",
    "# Print the feature ranking\n",
    "print(\"Feature ranking:\")\n",
    "\n",
    "for f in range(X.shape[1]):\n",
    "    print(\"%d. feature %s (%f)\" % (f + 1, feature_order[f], importances[indices[f]]))\n",
    "\n",
    "# Plot the feature importances of the forest\n",
    "plt.figure()\n",
    "plt.title(\"Feature importances\")\n",
    "plt.bar(range(X.shape[1]), importances[indices],\n",
    "       color=\"r\", align=\"center\")\n",
    "plt.xticks(range(X.shape[1]), feature_order, rotation='vertical')\n",
    "plt.xlim([-1, X.shape[1]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rmspe_xg(yhat, y):\n",
    "    # y = y.values\n",
    "    y = y.get_label()\n",
    "    y = np.exp(y) - 1\n",
    "    yhat = np.exp(yhat) - 1\n",
    "    w = ToWeight(y)\n",
    "    rmspe = np.sqrt(np.mean(w * (y - yhat)**2))\n",
    "    return \"rmspe\", rmspe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "train_x, eval_x, train_y, eval_y = train_test_split(X, y, test_size=0.25, random_state=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rmspe_xg' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-4e6b565f6f5d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mdtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDMatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mwatchlist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdvalid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'eval'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mgbm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_trees\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwatchlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearly_stopping_rounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m75\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrmspe_xg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgbm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'rmspe_xg' is not defined"
     ]
    }
   ],
   "source": [
    "params = {\"objective\": \"reg:linear\",\n",
    "            \"eta\": 0.02,\n",
    "            \"max_depth\": 10,\n",
    "            \"subsample\": 0.5,\n",
    "            \"colsample_bytree\": 0.7,\n",
    "            \"silent\": 1\n",
    "             }\n",
    "num_trees = 2000\n",
    "dtrain = xgb.DMatrix(X, y)\n",
    "dvalid = xgb.DMatrix(eval_x, eval_y)\n",
    "dtest = xgb.DMatrix(X_test)\n",
    "watchlist = [(dvalid, 'eval'), (dtrain, 'train')]\n",
    "gbm = xgb.train(params, dtrain, num_trees, evals=watchlist, early_stopping_rounds=75, feval=rmspe_xg, verbose_eval=True)\n",
    "prediction = gbm.predict(dtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predicted = pd.Series(clf.predict(X_test))\n",
    "solution = pd.concat([data_test['PassengerId'], predicted], axis=1)\n",
    "solution.columns = ['PassengerId','Survived']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "solution.to_csv('solution.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
